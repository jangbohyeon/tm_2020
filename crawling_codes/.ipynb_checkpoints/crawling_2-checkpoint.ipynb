{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome('./chromedriver.exe/')\n",
    "driver.implicitly_wait(3)\n",
    "driver.get('https://www.bigkinds.or.kr/')\n",
    "driver.implicitly_wait(1)\n",
    "\n",
    "def searchDate(sdate, edate):\n",
    "    #기간 창 클릭\n",
    "    driver.find_element_by_xpath('/html/body/div[10]/div[2]/div/form/div/div/div/div[3]/div[1]/button').click()\n",
    "\n",
    "    BACKSPACE = '/ue003'\n",
    "\n",
    "    start_date = driver.find_element_by_xpath('//input[@id=\"search-begin-date\"]')\n",
    "    #start date 넣기\n",
    "    for i in range(1, 11):\n",
    "        start_date.send_keys(Keys.BACKSPACE)\n",
    "    start_date.send_keys(sdate)\n",
    "\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    end_date = driver.find_element_by_xpath('//input[@id=\"search-end-date\"]')\n",
    "    #end date 넣기\n",
    "    for i in range(1, 11):\n",
    "        end_date.send_keys(Keys.BACKSPACE)\n",
    "    end_date.send_keys(edate)\n",
    "\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    #적용버튼 클릭\n",
    "    driver.find_element_by_xpath('/html/body/div[10]/div[2]/div/form/div/div/div/div[3]/div[1]/div/div[5]/button[2]').click()\n",
    "    driver.implicitly_wait(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchDate('2020-08-01', '2020-08-01')\n",
    "#검색버튼 클릭\n",
    "driver.find_element_by_xpath('/html/body/div[10]/div[2]/div/form/div/div/div/div[1]/span/button').click()\n",
    "driver.implicitly_wait(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#해당 날짜의 기사수가 몇개인지 확인. 기사갯수에 따라 달라지는 페이지수 확인. 페이지수에 따라 달라지는 반복횟수 확인\n",
    "news_cnt = str(driver.find_element_by_xpath('//*[@id=\"total-news-cnt\"]').text)\n",
    "news_cnt = news_cnt.replace(',', '')\n",
    "if int(news_cnt) % 10 == 0:\n",
    "    pages = int(news_cnt) // 10\n",
    "else:\n",
    "    pages = int(news_cnt) // 10 + 1\n",
    "ppages = math.ceil(pages / 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "#ppages-1번 반복\n",
    "for repeat in range(1, ppages):\n",
    "    #page는 2부터 8까지 클릭(반복)\n",
    "    for page in range(2, 9):\n",
    "        if page != pages:\n",
    "            #페이지마다 기사 10개씩 크롤링 -> text에 리스트로 저장\n",
    "            for i in range(1, 11):\n",
    "                #기사 클릭\n",
    "                path = '//*[@id=\"news-results\"]/div[' + str(i) + ']/div[2]/h4'\n",
    "                driver.find_element_by_xpath(path).click()\n",
    "\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                #기사의 내용 크롤링\n",
    "                tx = soup.select('#news-detail-modal > div > div > div.modal-body > div')\n",
    "                for k in tx:\n",
    "                    text.append(k.getText().strip())\n",
    "\n",
    "                #X버튼 클릭 -> 다시 다른 기사들이 있는 페이지로 이동\n",
    "                driver.find_element_by_xpath('//*[@id=\"news-detail-modal\"]/div/div/div[1]/button/span').click()\n",
    "            time.sleep(5)\n",
    "\n",
    "        else:\n",
    "            #해당 페이지에 기사가 10개 아닌 경우(news_cnt를 10으로 나눈 나머지)만큼만 크롤링 -> text에 리스트로 저장\n",
    "            n = int(news_cnt) % 10\n",
    "            for i in range(1, n + 1):\n",
    "                WebDriverWait(driver, 10)\n",
    "                path = '//*[@id=\"news-results\"]/div[' + str(i) + ']/div[2]/h4'\n",
    "                driver.find_element_by_xpath(path).click()\n",
    "\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                tx = soup.select('#news-detail-modal > div > div > div.modal-body > div')\n",
    "                for k in tx:\n",
    "                    text.append(k.getText().strip())\n",
    "\n",
    "                driver.find_element_by_xpath('//*[@id=\"news-detail-modal\"]/div/div/div[1]/button/span').click()\n",
    "            time.sleep(5)\n",
    "\n",
    "        #페이지 이동 클릭\n",
    "        page_path = '//*[@id=\"news-results-pagination\"]/ul/li[' + str(page + 2) + ']/a'\n",
    "        driver.find_element_by_xpath(page_path).click()\n",
    "        #페이지 이동 로딩이 걸릴 수 있으므로 sleep을 걸어준다.\n",
    "        time.sleep(10)\n",
    "\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#크롤링 완료 후 text를 엑셀파일로 저장 -> 추후에 다시 사용할 수 있도록\n",
    "import pandas as pd\n",
    "news_20200801 = pd.DataFrame(text)\n",
    "news_20200801.to_excel('./news_20200801.xlsx')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
