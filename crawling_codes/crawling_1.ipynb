{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from openpyxl import Workbook, load_workbook\n",
    "import datetime\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import reuters\n",
    "from pandas import DataFrame as df\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 불러오기\n",
    "file_20200801 = pd.read_excel('../data_files/NewsResult_20200801-20200801.xlsx',\
     sheet_name='sheet')\n",
    "columns_name = ['일자', '제목', '본문', '인물', '기관', '키워드', '특성추출(가중치순 상위 50개)',\
     '언론사', 'URL']\n",
    "file_20200801 = file_20200801[columns_name]\n",
    "news_20200801 = file_20200801\n",
    "news_20200801 = news_20200801.dropna(subset=['제목'])\n",
    "news_20200801 = news_20200801.dropna(subset=['URL'])\n",
    "news_20200801 = news_20200801.reset_index()\n",
    "\n",
    "news_tag = pd.read_excel('./path.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag : 딕셔너리형태\n",
    "#tag를 반복문을 통해 값을 추출하도록 진행\n",
    "#tag_row에 행번호가 입력\n",
    "tag_dict=news_tag.to_dict(orient=\"records\")\n",
    "\n",
    "tag_row = []\n",
    "for news_row in range(0, len(news_20200801)):\n",
    "    for tag in range(0,len(news_tag)):\n",
    "        if tag_dict[tag]['언론사'] == news_20200801['언론사'][news_row]:\n",
    "            tag_row += [tag]\n",
    "\n",
    "URL = news_20200801['URL'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.headless =True\n",
    "ti = []\n",
    "con = []\n",
    "for news_page in range(0,len(URL),1):\n",
    "    try:\n",
    "        url = URL[news_page]\n",
    "        driver = webdriver.Chrome('./chromedriver.exe', options=options)\n",
    "        driver.get(url)\n",
    "\n",
    "#            search_result = driver.find_element_by_xpath('#articletxt')\n",
    "#            search_result.click()\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        # tolist : 추출한 값 리스트로 변경하는 함수\n",
    "        title = soup.select(tag_dict[tag_row[news_page]]['title'])\n",
    "        time.sleep(2)\n",
    "        content = soup.select(tag_dict[tag_row[news_page]]['content'])\n",
    "\n",
    "        for n in title:\n",
    "            print(n.text.strip())\n",
    "            t = n.text.strip()\n",
    "            ti.append(t)\n",
    "        for n in content:\n",
    "            print(n.text.strip())\n",
    "            c = n.text.strip()\n",
    "            con.append(c)\n",
    "\n",
    "    except HTTPError as e:\n",
    "        pass\n",
    "\n",
    "    news_20200801 = pd.DataFrame([ti, con])\n",
    "    news_20200801.to_excel('news_20200801.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
