{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "def excel_reader():\n",
    "    # 해당 경로에 있는 .xlsx 포맷의 파일이름을 리스트로 가져오기\n",
    "    path = \"/home/u1026/tm_2020/\"\n",
    "    file_list = os.listdir(path)\n",
    "    exfile_list = [i for i in file_list if os.path.splitext(i)[-1]==\".xlsx\"]\n",
    "\n",
    "    # 파일이름을 split 해 해당 파일의 날짜 리스트로 가져오기\n",
    "    fname_list = []\n",
    "    for i in range(len(exfile_list)):\n",
    "        fname_list += [exfile_list[i].split(\"-\")[0].split(\"_\")[1]]\n",
    "\n",
    "    # exfile_list에 있는 엑셀파일 모두 읽어오기\n",
    "    for k in range(len(exfile_list)):\n",
    "        file_locate = path + exfile_list[k]\n",
    "        setattr(mod, 'file_{}'.format(fname_list[k]), pd.read_excel(file_locate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns_name = ['일자', '제목', '본문', '인물', '기관', '키워드', '특성추출(가중치순 상위 50개)', 'URL']\n",
    "file_20200801 = file_20200801[columns_name]\n",
    "file_20200802 = file_20200802[columns_name]\n",
    "\n",
    "news_20200801 = file_20200801\n",
    "news_20200802 = file_20200802\n",
    "\n",
    "#'제목'열에서 na가 있으면 해당 행 삭제\n",
    "news_20200801 = news_20200801.dropna(subset=['제목'])\n",
    "news_20200802 = news_20200802.dropna(subset=['제목'])\n",
    "\n",
    "#row번호 리셋하기\n",
    "news_20200801 = news_20200801.reset_index()\n",
    "news_20200802 = news_20200802.reset_index()\n",
    "\n",
    "\n",
    "#제목 마지막에 공백을 넣어 본문과 합칠 때 자연스럽게 합치게 함\n",
    "for i in range(0,len(news_20200801)):\n",
    "    news_20200801['제목'][i] = news_20200801['제목'][i] + ' '\n",
    "    \n",
    "for i in range(0,len(news_20200802)):\n",
    "    news_20200802['제목'][i] = news_20200802['제목'][i] + ' '\n",
    "\n",
    "\n",
    "news_20200801['내용'] = news_20200801.iloc[:, 2:4].sum(1)\n",
    "news_20200802['내용'] = news_20200802.iloc[:, 2:4].sum(1)\n",
    "\n",
    "for i in range(0,len(news_20200801)):\n",
    "    news_20200801['내용'][i] = news_20200801['내용'][i].replace(',','').replace('\\n','').replace('.','').replace('\"','').replace('!','').replace('(',' ').replace(')','').replace('?','').casefold()\n",
    "#숫자 지우기\n",
    "#news_20200801['내용'] = news_20200801['내용'].str.replace('[0-9]', '')\n",
    "\n",
    "for i in range(0,len(news_20200802)):\n",
    "    news_20200802['내용'][i] = news_20200802['내용'][i].replace(',','').replace('\\n','').replace('.','').replace('\"','').replace('!','').replace('(',' ').replace(')','').replace('?','').casefold()\n",
    "#news_20200802['내용'] = news_20200802['내용'].str.replace('[0-9]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_tokenize : 단어 기준으로 토큰화(Tokenize)\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_20200801 = []\n",
    "for i in range(len(news_20200801)):\n",
    "    word_20200801 += (word_20200801, word_tokenize(news_20200801['내용'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "#메모리 늘려주기\n",
    "komoran = Komoran(max_heap_size=1024 * 6)\n",
    "\n",
    "km_word_20200801 = []\n",
    "for i in range(len(news_20200801)):\n",
    "    #print(okt.morphs(news_190801['내용'][i]))\n",
    "    km_word_20200801 +=  komoran.nouns(news_20200801['내용'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "with open('./data_files/stopwords_list1.txt', 'r') as file1, \\\n",
    "open('./data_files/stopwords_list2.txt', 'r') as file2:\n",
    "    for text1 in file1:\n",
    "        stop_words.append(text1.strip('\\n'))\n",
    "    print(len(stop_words))\n",
    "    for text2_1 in file2:\n",
    "        text2 = text2_1.split('\\t')\n",
    "        stop_words.append(text2[0].strip('\\n'))\n",
    "    print(len(stop_words))\n",
    "\n",
    "#불용어리스트 생성\n",
    "stopwords_set = set(stop_words)\n",
    "stop_words = list(stopwords_set)\n",
    "\n",
    "\n",
    "#불용어 제거하기\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#komoran 사용할 때\n",
    "word_result_20200801 = []\n",
    "for w in km_word_20200801:\n",
    "    if w not in stop_words:\n",
    "        word_result_20200801.append(w)\n",
    "\n",
    "print(\"----불용어 제거 전----\")\n",
    "print(len(km_word_20200801))\n",
    "print(\"----불용어 제거 후----\")\n",
    "print(len(word_result_20200801))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary 만들기\n",
    "print(len(word_result_20200801))\n",
    "set_20200801 = set(word_result_20200801)\n",
    "voca_20200801 = list(set_20200801)\n",
    "\n",
    "#vocabulary 한글자이면 지우기\n",
    "for i in voca_20200801:\n",
    "    if len(i) == 1:\n",
    "        voca_20200801.remove(i)\n",
    "\n",
    "print(len(voca_20200801))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DTM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "dtm_ar_20200801 = cv.fit_transform(voca_20200801).toarray()\n",
    "feature_names = cv.get_feature_names()\n",
    "#dtm을 데이터프레임 형태로\n",
    "dtm_20200801 = pd.DataFrame(dtm_ar_20200801, columns=feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
